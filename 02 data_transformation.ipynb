{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Documentation\n",
    "- <https://praw.readthedocs.io/en/latest/>\n",
    "\n",
    "#### 2. Examples\n",
    "- <https://praw.readthedocs.io/en/latest/tutorials/comments.html>\n",
    "- <https://www.sciencedirect.com/topics/computer-science/centrality-measure>\n",
    "- <https://aksakalli.github.io/2017/07/17/network-centrality-measures-and-their-visualization.html>\n",
    "- <https://cambridge-intelligence.com/keylines-faqs-social-network-analysis/>\n",
    "- <https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data trancformation\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Reddit extraction\n",
    "import praw\n",
    "\n",
    "#Time \n",
    "import time\n",
    "\n",
    "# graph packages\n",
    "import networkx as nx\n",
    "import igraph\n",
    "from igraph import Graph\n",
    "\n",
    "# NLP\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='pG_dvbbdt151Rw', \\\n",
    "                     client_secret='KNp35NWOZ7FvjyF5haiPUAKSd8o', \\\n",
    "                     user_agent='O_Auth_app', \\\n",
    "                     username='_Alex_shadow_', \\\n",
    "                     password='2424678Rise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(database_list):\n",
    "    \n",
    "    conn = sqlite3.connect('{0}.db'.format(database_list))\n",
    "    \n",
    "    # 1) Preparing a list of post ids\n",
    "    list_of_posts = pd.read_sql('''SELECT distinct post_id\n",
    "                                    FROM Reddit\n",
    "                                    GROUP BY post_id\n",
    "                                    ORDER BY  max(score) desc\n",
    "                                    LIMIT 110''', conn).values.tolist()\n",
    "    \n",
    "    # 2) Empty dictionary\n",
    "    topics_dict = {\"author\": [],\n",
    "                   \"body\":[],\n",
    "                   \"created_utc\":[],\n",
    "                   \"id\": [],\n",
    "                   \"post_id\": [],\n",
    "                   \"parent_id\": [],\n",
    "                   \"score\": [],\n",
    "                   \"subreddit_id\": [],\n",
    "                   \"num_comments\": []               \n",
    "                  }\n",
    "    \n",
    "    # 3) Extracting posts\n",
    "    for elements in list_of_posts:\n",
    "        submission  = reddit.submission(id=str(elements).strip('[').strip(']').strip(\"'\"))\n",
    "\n",
    "        topics_dict[\"author\"].append(submission.author)\n",
    "        topics_dict[\"body\"].append(submission.title+\" \"+submission.selftext)\n",
    "        topics_dict[\"created_utc\"].append(submission.created_utc)\n",
    "        topics_dict[\"id\"].append(submission.id)\n",
    "        topics_dict[\"post_id\"].append(submission.name)\n",
    "        topics_dict[\"parent_id\"].append(submission.name)\n",
    "        topics_dict[\"score\"].append(submission.score)\n",
    "        topics_dict[\"subreddit_id\"].append(submission.subreddit_id)\n",
    "        topics_dict[\"num_comments\"].append(submission.num_comments)\n",
    "        \n",
    "    df_post=pd.DataFrame.from_dict(topics_dict, orient='index').transpose()\n",
    "    df_post['type']=\"Post\"\n",
    "    df_post[\"subreddit_id\"]=df_post[\"subreddit_id\"].str[3:]    \n",
    "    df_post[\"parent_id\"]=df_post[\"parent_id\"].str[3:]\n",
    "    df_post[\"post_id\"]=df_post[\"post_id\"].str[3:]\n",
    "    \n",
    "    # 4) Extracting comments\n",
    "    df_comments = pd.read_sql('''SELECT\n",
    "                                    author,\n",
    "                                    body,\n",
    "                                    created_utc,\n",
    "                                    id,\n",
    "                                    post_id,\n",
    "                                    parent_id,\n",
    "                                    score,\n",
    "                                    subreddit_id\n",
    "\n",
    "                                 FROM Reddit\n",
    "                                 WHERE post_id in (SELECT \n",
    "                                                        distinct post_id\n",
    "                                                    FROM Reddit\n",
    "                                                    GROUP BY post_id\n",
    "                                                    ORDER BY  max(score) desc \n",
    "                                                    LIMIT 110)\n",
    "                                 ''', conn)\n",
    "\n",
    "    df_comments['type']=\"Comments\"    \n",
    "    \n",
    "    # 5) Collect all together\n",
    "    df_all = pd.concat([df_comments, df_post], join='outer', sort=False).replace({np.nan: None}).reset_index(drop=True)\n",
    "\n",
    "    return df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation of statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(df):\n",
    "\n",
    "    # list of posts\n",
    "    list_of_posts=list(pd.unique(df[\"post_id\"].values))\n",
    "    \n",
    "    # Empty dataframe\n",
    "    df_merge = pd.DataFrame()\n",
    "    \n",
    "    # cycling through posts \n",
    "    for elements in list_of_posts:\n",
    "        \n",
    "        # filter dataset \n",
    "        df_filter=df.loc[df['post_id'] == elements].copy().sort_values(by=['id'], ascending=False)\n",
    "\n",
    "        G = nx.from_pandas_edgelist(df=df_filter, source=\"id\", target=\"parent_id\", create_using=nx.Graph()) \n",
    "        if nx.is_connected(G)==False:\n",
    "            print(\"Dropped: {}\".format(elements))\n",
    "            continue \n",
    "            \n",
    "            \n",
    "        # 1. Calculate level depth\n",
    "        depth_levels_from_root = pd.DataFrame(list(nx.single_source_shortest_path_length(G, source=elements).items()), columns=['id', 'level']).sort_values(by=['id'], ascending=False)\n",
    "        \n",
    "        # 2. Calculate number of connected nodes\n",
    "        n_connections = pd.DataFrame(list( dict(G.degree()).items() ), columns=['id', 'n_conn']).sort_values(by=['id'], ascending=False)\n",
    "        \n",
    "        # 2. Add number of comments and month\n",
    "        df_filter[\"num_comments\"]=df_filter.loc[df_filter['type'] == 'Post', \"num_comments\"].values[0]\n",
    "        \n",
    "        # 3. finding how many seconds passed for post\n",
    "        def get_yyyy_mm_dd_from_utc(dt):\n",
    "            date = datetime.utcfromtimestamp(dt) \n",
    "            return date \n",
    "        df_filter['date'] = df_filter['created_utc'].astype('int').apply(lambda d: get_yyyy_mm_dd_from_utc(d))   \n",
    "\n",
    "        # 4. Adding month and quarter labels\n",
    "        df_filter['year'] = df_filter['date'].dt.year \n",
    "        df_filter['month'] = df_filter['date'].dt.month \n",
    "        df_filter['quarter'] = df_filter['date'].dt.quarter\n",
    "        \n",
    "        # 5. Grouping by time\n",
    "        t_min = min(df_filter[\"date\"])\n",
    "        t_max = max(df_filter[\"date\"])\n",
    "        diff=t_max-t_min\n",
    "        t_diff=diff.days*24\n",
    "        time_int=(t_diff,730,168,24,16,8)\n",
    "        label=(\"<=8 hours\",\"<=16 hours\",\"<=1 day\",\"<=7 days\",\"<=1 month\",\"more than 1 month\")\n",
    "        num=5\n",
    "        for elements in time_int:\n",
    "            t = t_min + dt.timedelta(hours=elements) \n",
    "            df_filter.loc[df_filter[\"date\"] <= t, 'time_group'] = num \n",
    "            df_filter.loc[df_filter[\"date\"] <= t, 'time_label'] = label[num] \n",
    "            num-=1\n",
    "        df_filter['time_group']=df_filter['time_group'].astype('int8')\n",
    "                \n",
    "        # 6. Adding info about previous author               \n",
    "        df_user=df_filter[[\"id\",\"author\",\"date\"]]\n",
    "        df_user = df_user.rename(columns={'author': 'parent_author',\n",
    "                                        'id': 'parent_id',\n",
    "                                        'date': 'parent_date'})\n",
    "        df_filter=pd.merge(df_filter, df_user, on='parent_id')\n",
    "        \n",
    "        df_filter['age_comment'] = (df_filter.date - df_filter.parent_date).dt.total_seconds().astype('str')\n",
    "        df_filter['age_post'] = (df_filter.date - min(df_filter.date)).dt.total_seconds().astype('str')\n",
    "               \n",
    "        # 7. Count number of text in each comment\n",
    "        df_filter['body_length']=df_filter['body'].str.len()\n",
    "        \n",
    "        # append data\n",
    "        df_filter=pd.merge(df_filter, depth_levels_from_root, on='id')       \n",
    "        df_filter=pd.merge(df_filter, n_connections, on='id')\n",
    "        df_merge=df_merge.append(df_filter, sort=True)\n",
    "        \n",
    "    return df_merge    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of centrality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centrality_measures(df):\n",
    "\n",
    "    # list of posts\n",
    "    list_of_posts=list(pd.unique(df[\"post_id\"].values))\n",
    "    \n",
    "    # Empty dataframe\n",
    "    df_merge = pd.DataFrame()\n",
    "\n",
    "    # cycling through posts \n",
    "    for elements in list_of_posts:\n",
    "      \n",
    "        # filter dataset \n",
    "        df_filter=df.loc[df['post_id'] == elements].copy().sort_values(by=['id'], ascending=False)\n",
    "        \n",
    "        # Directed graph\n",
    "        G = nx.from_pandas_edgelist(df=df_filter, source=\"id\", target=\"parent_id\", create_using=nx.DiGraph()) \n",
    "        \n",
    "        # 1. Calculate page rank\n",
    "        pr = pd.DataFrame(list(nx.pagerank(G, alpha=0.8).items()), columns=['id', 'centrality_page_rank']).sort_values(by=['id'], ascending=False)  \n",
    "        \n",
    "        # 2. Calculate eigenvector\n",
    "        eigenvector = pd.DataFrame(list(nx.eigenvector_centrality_numpy(G).items()), columns=['id', 'centrality_eigenvector']).sort_values(by=['id'], ascending=False)  \n",
    "        \n",
    "        # 3. Calculate degree centrality\n",
    "        degree = pd.DataFrame(list(nx.degree_centrality(G).items()), columns=['id', 'centrality_degree']).sort_values(by=['id'], ascending=False)  \n",
    "        \n",
    "        # 4. Calculate closeness centrality\n",
    "        closeness = pd.DataFrame(list(nx.closeness_centrality(G).items()), columns=['id', 'centrality_closeness']).sort_values(by=['id'], ascending=False)  \n",
    "         \n",
    "        # Directed graph\n",
    "        g = Graph()\n",
    "        tuples = [tuple(x) for x in df_filter[[\"id\", \"parent_id\"]].values]\n",
    "        G = igraph.Graph.TupleList(tuples, directed = True)\n",
    "        \n",
    "        # 5. Calculate betweeness centrality\n",
    "        df_filter[\"centrality_betweeness\"]= [float(i)/max(G.betweenness()) for i in G.betweenness()]\n",
    "\n",
    "        # Merge all together\n",
    "        df_filter=pd.merge(df_filter, pr, on='id')          \n",
    "        df_filter=pd.merge(df_filter, eigenvector, on='id')    \n",
    "        df_filter=pd.merge(df_filter, degree, on='id')    \n",
    "        df_filter=pd.merge(df_filter, closeness, on='id')   \n",
    "        \n",
    "        # append data\n",
    "        df_merge=df_merge.append(df_filter, sort=True)\n",
    "    \n",
    "    return df_merge    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # removing hyperlinks\n",
    "    phrase = re.sub(r'\\b(?:(?:https?|ftp)://)?\\w[\\w-]*(?:\\.[\\w-]+)+\\S*', ' ', phrase)\n",
    "    phrase = re.sub(r\"http:\", \"\", phrase)\n",
    "    \n",
    "    # dropping unwanted symbols:\n",
    "    phrase = re.sub(r\"&lt;\", \"\", phrase)\n",
    "    phrase = re.sub(r\"&gt;\", \"\", phrase)\n",
    "    phrase = re.sub(r\"&le;\", \"\", phrase)\n",
    "    phrase = re.sub(r\"&ge;\", \"\", phrase)\n",
    "\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_scoring(df):\n",
    "    sia = SIA()\n",
    "    results = []\n",
    "\n",
    "    for line in df['body']:\n",
    "        pol_score = sia.polarity_scores(line)\n",
    "        results.append(pol_score)\n",
    "        \n",
    "    df_text = pd.DataFrame.from_records(results)\n",
    "    \n",
    "    #Assign label and nlp score\n",
    "    df['nlp_score'] = df_text['compound']\n",
    "   \n",
    "\n",
    "    df['nlp_group'] = 4\n",
    "    df['nlp_label'] = \"Very Positive\" \n",
    "\n",
    "    df.loc[df[\"nlp_score\"] <= 0.5, 'nlp_group'] = 3\n",
    "    df.loc[df[\"nlp_score\"] <= 0.5, 'nlp_label'] = \"Positive\" \n",
    "\n",
    "    df.loc[df[\"nlp_score\"] <= 0.05, 'nlp_group'] = 2\n",
    "    df.loc[df[\"nlp_score\"] <= 0.05, 'nlp_label'] = \"Neutral\" \n",
    "\n",
    "    df.loc[df[\"nlp_score\"] <= -0.05, 'nlp_group'] = 1\n",
    "    df.loc[df[\"nlp_score\"] <= -0.05, 'nlp_label'] = \"Negative\" \n",
    "\n",
    "    df.loc[df[\"nlp_score\"] <= -0.5, 'nlp_group'] = 0 \n",
    "    df.loc[df[\"nlp_score\"] <= -0.5, 'nlp_label'] = \"Very Negative\" \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_formatting(df):\n",
    "\n",
    "    # post info\n",
    "    df['author']=df['author'].astype('str')  \n",
    "    df['body']=df['body'].astype('str')\n",
    "    df['id']=df['id'].astype('str')\n",
    "    df['post_id']=df['post_id'].astype('str')\n",
    "    df['subreddit_id']=df['subreddit_id'].astype('str')\n",
    "    \n",
    "    df['score']=df['score'].astype('int16')\n",
    "    df['type']=df['type'].astype('str')\n",
    "    df['level']=df['level'].astype('int16')\n",
    "    df['n_conn']=df['n_conn'].astype('int16')\n",
    "    df['num_comments']=df['num_comments'].astype('int16')\n",
    "    df['body_length']=df['body_length'].astype('int16')\n",
    "    \n",
    "    # centrality measures\n",
    "    df['centrality_betweeness']=df['centrality_betweeness'].round(6).astype('float16')\n",
    "    df['centrality_closeness']=df['centrality_closeness'].round(6).astype('float16')\n",
    "    df['centrality_degree']=df['centrality_degree'].round(6).astype('float16')  \n",
    "    df['centrality_eigenvector']=df['centrality_eigenvector'].round(6).astype('float16')\n",
    "    df['centrality_page_rank']=df['centrality_page_rank'].round(6).astype('float16')\n",
    "    \n",
    "    #time \n",
    "    df['created_utc']=df['created_utc'].astype('int16')\n",
    "    df['date']=df['date'].astype('str')\n",
    "    \n",
    "    df['month']=df['month'].astype('int16')\n",
    "    df['year']=df['year'].astype('int16')\n",
    "    df['quarter']=df['quarter'].astype('int16')  \n",
    "    \n",
    "    df['age_comment']=df['age_comment'].astype('str')\n",
    "    df['age_post']=df['age_post'].astype('str')\n",
    "        \n",
    "    df['time_group']=df['time_group'].astype('int16')    \n",
    "    df['time_label']=df['time_label'].astype('str')  \n",
    "    \n",
    "    #previous post\n",
    "    df['parent_author']=df['parent_author'].astype('str')\n",
    "    df['parent_id']=df['parent_id'].astype('str')\n",
    "    df['parent_date']=df['parent_date'].astype('str')    \n",
    "\n",
    "    #NLP\n",
    "    df['nlp_score']=df['nlp_score'].round(6).astype('float16')\n",
    "    df['nlp_group']=df['nlp_group'].astype('int8')\n",
    "    df['nlp_label']=df['nlp_label'].astype('str')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_list, out_database):\n",
    "    \n",
    "    # Empty dataframe\n",
    "    df_merge = pd.DataFrame()\n",
    "\n",
    "    start_time = time.time()   \n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"Start time: {}\".format(start_time))\n",
    "    conn = sqlite3.connect(out_database)\n",
    "    for elements in data_list:\n",
    "        print(\"---------------------------------------------------\")\n",
    "        print(\"Database : {}\".format(elements))       \n",
    "        step0 = time.time()\n",
    "        # 1) Exracting comments\n",
    "        df_1 = data_extraction(elements)\n",
    "        step1 = time.time()\n",
    "\n",
    "        print(\"Step 1 done in: {}\".format(step1-step0))\n",
    "        \n",
    "        # 2) Preparing data\n",
    "        df_2 = data_preparation(df_1)\n",
    "        step2 = time.time()\n",
    "        print(\"Step 2 done in: {}\".format(step2-step1))\n",
    "        \n",
    "        # 3) centrality calculation\n",
    "        df_3 = centrality_measures(df_2)\n",
    "        step3 = time.time()  \n",
    "        print(\"Step 3 done in: {}\".format(step3-step2))\n",
    "        \n",
    "        # 4) NLP analysis\n",
    "        df_4 = nlp_scoring(df_3)\n",
    "        step4 = time.time()\n",
    "        print(\"Step 4 done in: {}\".format(step4-step3))\n",
    "        \n",
    "        # 5) data format\n",
    "        df_5 = data_formatting(df_4)\n",
    "        step5 = time.time()\n",
    "        print(\"Step 5 done in: {}\".format(step5-step4))\n",
    "        \n",
    "        df_5.to_sql('Reddit', conn, if_exists='append', index=False)\n",
    "        #df_5.to_sql('Reddit', conn, if_exists='replace', index=False)\n",
    "        \n",
    "    print(\"Total time: {}\".format(time.time()-start_time))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of datasets to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "Start time: 1594742899.9225092\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2016/10_2016\n",
      "Step 1 done in: 649.4753940105438\n",
      "Step 2 done in: 46.22297739982605\n",
      "Step 3 done in: 1853.0605521202087\n",
      "Step 4 done in: 198.1242334842682\n",
      "Step 5 done in: 2.5012917518615723\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2016/11_2016\n",
      "Step 1 done in: 779.5968918800354\n",
      "Step 2 done in: 64.56959295272827\n",
      "Step 3 done in: 2177.4735159873962\n",
      "Step 4 done in: 264.6592037677765\n",
      "Step 5 done in: 3.468505620956421\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2016/12_2016\n",
      "Step 1 done in: 731.5279066562653\n",
      "Step 2 done in: 55.55265522003174\n",
      "Step 3 done in: 2003.5390720367432\n",
      "Step 4 done in: 261.5582118034363\n",
      "Step 5 done in: 3.5097899436950684\n",
      "Total time: 9113.05395936966\n"
     ]
    }
   ],
   "source": [
    "data_list=[\"Data/initial/2016/10_2016\",\n",
    "           \"Data/initial/2016/11_2016\",\n",
    "           \"Data/initial/2016/12_2016\"]\n",
    "data_prep(data_list, out_database=\"Data/trump_election.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "Start time: 1594752013.378426\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/01_2012\n",
      "Step 1 done in: 648.3846743106842\n",
      "Dropped: nxu96\n",
      "Dropped: o0dm1\n",
      "Dropped: o3cc7\n",
      "Step 2 done in: 18.137250661849976\n",
      "Step 3 done in: 183.8024160861969\n",
      "Step 4 done in: 79.60364484786987\n",
      "Step 5 done in: 1.005279541015625\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/02_2012\n",
      "Step 1 done in: 675.2601225376129\n",
      "Dropped: p8i1n\n",
      "Dropped: p9m43\n",
      "Dropped: pa3yr\n",
      "Dropped: pb9nb\n",
      "Dropped: pbu0z\n",
      "Dropped: pbv6n\n",
      "Dropped: pc2nm\n",
      "Dropped: pcato\n",
      "Dropped: pd9rv\n",
      "Dropped: pdbrs\n",
      "Dropped: pdc1y\n",
      "Dropped: pddgr\n",
      "Dropped: pexk3\n",
      "Step 2 done in: 15.499390602111816\n",
      "Step 3 done in: 144.71490097045898\n",
      "Step 4 done in: 75.06330585479736\n",
      "Step 5 done in: 0.8365685939788818\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/03_2012\n",
      "Step 1 done in: 652.6654098033905\n",
      "Dropped: qwgca\n",
      "Dropped: qxo6u\n",
      "Step 2 done in: 19.05805253982544\n",
      "Step 3 done in: 192.41818928718567\n",
      "Step 4 done in: 95.26238870620728\n",
      "Step 5 done in: 0.986558198928833\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/04_2012\n",
      "Step 1 done in: 619.8960313796997\n",
      "Dropped: shpt9\n",
      "Step 2 done in: 19.023414611816406\n",
      "Step 3 done in: 169.24428606033325\n",
      "Step 4 done in: 83.1001648902893\n",
      "Step 5 done in: 0.9321401119232178\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/05_2012\n",
      "Step 1 done in: 656.7606225013733\n",
      "Dropped: t0a92\n",
      "Dropped: t4d90\n",
      "Dropped: t6pjs\n",
      "Dropped: u76j9\n",
      "Dropped: ub55n\n",
      "Dropped: uc1l6\n",
      "Step 2 done in: 21.280945539474487\n",
      "Step 3 done in: 202.39483618736267\n",
      "Step 4 done in: 92.65547919273376\n",
      "Step 5 done in: 1.1830098628997803\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/06_2012\n",
      "Step 1 done in: 619.9062216281891\n",
      "Dropped: uej6z\n",
      "Dropped: uxsey\n",
      "Dropped: vklf8\n",
      "Step 2 done in: 32.215014934539795\n",
      "Step 3 done in: 367.90771484375\n",
      "Step 4 done in: 119.97222423553467\n",
      "Step 5 done in: 1.249448537826538\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/07_2012\n",
      "Step 1 done in: 677.9429569244385\n",
      "Dropped: wvwu9\n",
      "Dropped: wwfy0\n",
      "Step 2 done in: 29.110843420028687\n",
      "Step 3 done in: 307.28288745880127\n",
      "Step 4 done in: 125.87902998924255\n",
      "Step 5 done in: 1.3366036415100098\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/08_2012\n",
      "Step 1 done in: 657.8035073280334\n",
      "Step 2 done in: 26.059637308120728\n",
      "Step 3 done in: 337.2730848789215\n",
      "Step 4 done in: 134.63318252563477\n",
      "Step 5 done in: 1.562244176864624\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/09_2012\n",
      "Step 1 done in: 670.778480052948\n",
      "Dropped: ze2kx\n",
      "Dropped: ze9zn\n",
      "Dropped: zl0vd\n",
      "Step 2 done in: 23.04581594467163\n",
      "Step 3 done in: 263.6239981651306\n",
      "Step 4 done in: 106.47012662887573\n",
      "Step 5 done in: 1.1937048435211182\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/10_2012\n",
      "Step 1 done in: 653.2737333774567\n",
      "Step 2 done in: 28.379010438919067\n",
      "Step 3 done in: 494.4772539138794\n",
      "Step 4 done in: 129.5031545162201\n",
      "Step 5 done in: 1.5121679306030273\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/11_2012\n",
      "Step 1 done in: 657.8660752773285\n",
      "Dropped: 13ia8q\n",
      "Dropped: 13lkks\n",
      "Step 2 done in: 27.07042670249939\n",
      "Step 3 done in: 430.00304341316223\n",
      "Step 4 done in: 123.94936180114746\n",
      "Step 5 done in: 1.498464584350586\n",
      "---------------------------------------------------\n",
      "Database : Data/initial/2012/12_2012\n",
      "Step 1 done in: 647.2658696174622\n",
      "Dropped: 14uoel\n",
      "Dropped: 158nuq\n",
      "Step 2 done in: 21.269616842269897\n",
      "Step 3 done in: 220.81377792358398\n",
      "Step 4 done in: 104.40282797813416\n",
      "Step 5 done in: 1.176307201385498\n",
      "Total time: 12742.18431353569\n"
     ]
    }
   ],
   "source": [
    "data_list=[\"Data/initial/2012/01_2012\",\n",
    "           \"Data/initial/2012/02_2012\",\n",
    "           \"Data/initial/2012/03_2012\",\n",
    "           \"Data/initial/2012/04_2012\",\n",
    "           \"Data/initial/2012/05_2012\",\n",
    "           \"Data/initial/2012/06_2012\",\n",
    "           \"Data/initial/2012/07_2012\",\n",
    "           \"Data/initial/2012/08_2012\",\n",
    "           \"Data/initial/2012/09_2012\",\n",
    "           \"Data/initial/2012/10_2012\",\n",
    "           \"Data/initial/2012/11_2012\",\n",
    "           \"Data/initial/2012/12_2012\"]\n",
    "data_prep(data_list, out_database=\"Data/obama_election.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_comment</th>\n",
       "      <th>age_post</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>body_length</th>\n",
       "      <th>centrality_betweeness</th>\n",
       "      <th>centrality_closeness</th>\n",
       "      <th>centrality_degree</th>\n",
       "      <th>centrality_eigenvector</th>\n",
       "      <th>centrality_page_rank</th>\n",
       "      <th>...</th>\n",
       "      <th>quarter</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>time_group</th>\n",
       "      <th>time_label</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>nlp_score</th>\n",
       "      <th>nlp_group</th>\n",
       "      <th>nlp_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2163819.0</td>\n",
       "      <td>2206052.0</td>\n",
       "      <td>Aegisflame</td>\n",
       "      <td>Actually then we're almost on the same page. M...</td>\n",
       "      <td>145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;=1 month</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.612305</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11032.0</td>\n",
       "      <td>326269.0</td>\n",
       "      <td>LucidicShadow</td>\n",
       "      <td>Interesting read. A little anecdotal, but inte...</td>\n",
       "      <td>69</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.612793</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34897.0</td>\n",
       "      <td>315237.0</td>\n",
       "      <td>cuntipede</td>\n",
       "      <td>It can return a positive result when you smoke...</td>\n",
       "      <td>183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.557617</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>252185.00000000003</td>\n",
       "      <td>303169.0</td>\n",
       "      <td>tacosaladchupacabra</td>\n",
       "      <td>&amp;gt;Checkpoints are constitutional because a l...</td>\n",
       "      <td>152</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.051605</td>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>819.0</td>\n",
       "      <td>280340.0</td>\n",
       "      <td>LucidicShadow</td>\n",
       "      <td>Ok, decided to look this shit up.\\n\\nAccording...</td>\n",
       "      <td>1188</td>\n",
       "      <td>0.033722</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.891602</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>239747.00000000003</td>\n",
       "      <td>279521.0</td>\n",
       "      <td>cuntipede</td>\n",
       "      <td>Ticket you? LOL. It is a full-blown drug drivi...</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.352539</td>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42012.0</td>\n",
       "      <td>272982.0</td>\n",
       "      <td>cjlm</td>\n",
       "      <td>If you refuse a breath test here, you get auto...</td>\n",
       "      <td>232</td>\n",
       "      <td>0.016861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.202271</td>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>186140.0</td>\n",
       "      <td>231073.0</td>\n",
       "      <td>rivermonkey66</td>\n",
       "      <td>We have a group called MADD which lied to and ...</td>\n",
       "      <td>136</td>\n",
       "      <td>0.043365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.659668</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>185682.0</td>\n",
       "      <td>230970.0</td>\n",
       "      <td>rivermonkey66</td>\n",
       "      <td>People refuse, the evidence is not clear, and ...</td>\n",
       "      <td>138</td>\n",
       "      <td>0.048187</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.200073</td>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>174026.0</td>\n",
       "      <td>230349.0</td>\n",
       "      <td>rivermonkey66</td>\n",
       "      <td>Missing the point.  *This should not exist, 20...</td>\n",
       "      <td>61</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.295898</td>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age_comment   age_post               author  \\\n",
       "0           2163819.0  2206052.0           Aegisflame   \n",
       "1             11032.0   326269.0        LucidicShadow   \n",
       "2             34897.0   315237.0            cuntipede   \n",
       "3  252185.00000000003   303169.0  tacosaladchupacabra   \n",
       "4               819.0   280340.0        LucidicShadow   \n",
       "5  239747.00000000003   279521.0            cuntipede   \n",
       "6             42012.0   272982.0                 cjlm   \n",
       "7            186140.0   231073.0        rivermonkey66   \n",
       "8            185682.0   230970.0        rivermonkey66   \n",
       "9            174026.0   230349.0        rivermonkey66   \n",
       "\n",
       "                                                body  body_length  \\\n",
       "0  Actually then we're almost on the same page. M...          145   \n",
       "1  Interesting read. A little anecdotal, but inte...           69   \n",
       "2  It can return a positive result when you smoke...          183   \n",
       "3  &gt;Checkpoints are constitutional because a l...          152   \n",
       "4  Ok, decided to look this shit up.\\n\\nAccording...         1188   \n",
       "5  Ticket you? LOL. It is a full-blown drug drivi...          108   \n",
       "6  If you refuse a breath test here, you get auto...          232   \n",
       "7  We have a group called MADD which lied to and ...          136   \n",
       "8  People refuse, the evidence is not clear, and ...          138   \n",
       "9  Missing the point.  *This should not exist, 20...           61   \n",
       "\n",
       "   centrality_betweeness  centrality_closeness  centrality_degree  \\\n",
       "0               0.000000              0.000000           0.000937   \n",
       "1               0.026505              0.000000           0.000937   \n",
       "2               0.000000              0.000937           0.001874   \n",
       "3               0.019272              0.000000           0.000937   \n",
       "4               0.033722              0.001250           0.001874   \n",
       "5               0.000000              0.001406           0.001874   \n",
       "6               0.016861              0.000000           0.000937   \n",
       "7               0.043365              0.000000           0.000937   \n",
       "8               0.048187              0.000937           0.001874   \n",
       "9               0.000000              0.000000           0.000937   \n",
       "\n",
       "   centrality_eigenvector  centrality_page_rank  ...  quarter score  \\\n",
       "0                     0.0              0.000187  ...        1     1   \n",
       "1                     0.0              0.000187  ...        1     1   \n",
       "2                     0.0              0.000337  ...        1     1   \n",
       "3                     0.0              0.000187  ...        1     1   \n",
       "4                     0.0              0.000457  ...        1     1   \n",
       "5                     0.0              0.000553  ...        1     1   \n",
       "6                     0.0              0.000187  ...        1     1   \n",
       "7                     0.0              0.000187  ...        1     0   \n",
       "8                     0.0              0.000337  ...        1     0   \n",
       "9                     0.0              0.000187  ...        1     1   \n",
       "\n",
       "  subreddit_id  time_group  time_label      type  year nlp_score nlp_group  \\\n",
       "0        2cneq           4   <=1 month  Comments  2016 -0.612305         3   \n",
       "1        2cneq           3    <=7 days  Comments  2016  0.612793         1   \n",
       "2        2cneq           3    <=7 days  Comments  2016  0.557617         1   \n",
       "3        2cneq           3    <=7 days  Comments  2016 -0.051605         2   \n",
       "4        2cneq           3    <=7 days  Comments  2016 -0.891602         3   \n",
       "5        2cneq           3    <=7 days  Comments  2016 -0.352539         2   \n",
       "6        2cneq           3    <=7 days  Comments  2016 -0.202271         2   \n",
       "7        2cneq           3    <=7 days  Comments  2016 -0.659668         3   \n",
       "8        2cneq           3    <=7 days  Comments  2016 -0.200073         2   \n",
       "9        2cneq           3    <=7 days  Comments  2016 -0.295898         2   \n",
       "\n",
       "  nlp_label  \n",
       "0  Negative  \n",
       "1  Positive  \n",
       "2  Positive  \n",
       "3   Neutral  \n",
       "4  Negative  \n",
       "5   Neutral  \n",
       "6   Neutral  \n",
       "7  Negative  \n",
       "8   Neutral  \n",
       "9   Neutral  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conn = sqlite3.connect(\"test.db\")\n",
    "conn = sqlite3.connect(\"Data/trump_election.db\")\n",
    "df_test = pd.read_sql('''SELECT * FROM Reddit limit 10''', conn)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_comment</th>\n",
       "      <th>age_post</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>body_length</th>\n",
       "      <th>centrality_betweeness</th>\n",
       "      <th>centrality_closeness</th>\n",
       "      <th>centrality_degree</th>\n",
       "      <th>centrality_eigenvector</th>\n",
       "      <th>centrality_page_rank</th>\n",
       "      <th>...</th>\n",
       "      <th>quarter</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>time_group</th>\n",
       "      <th>time_label</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>nlp_score</th>\n",
       "      <th>nlp_group</th>\n",
       "      <th>nlp_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TruthToPower1</td>\n",
       "      <td>Congress is at 5% approval, and 86% say Congre...</td>\n",
       "      <td>101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.319092</td>\n",
       "      <td>0.292480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.551270</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2510</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=8 hours</td>\n",
       "      <td>Post</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29579.000000000004</td>\n",
       "      <td>1437407.0</td>\n",
       "      <td>wharpudding</td>\n",
       "      <td>Hey, you're right.  We could have gotten rid o...</td>\n",
       "      <td>132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;=1 month</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.763184</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29409.000000000004</td>\n",
       "      <td>1437237.0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.020828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;=1 month</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1365677.0</td>\n",
       "      <td>1407916.0</td>\n",
       "      <td>sixpackistan</td>\n",
       "      <td>There are no term limits and, no, they're not ...</td>\n",
       "      <td>269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;=1 month</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2012</td>\n",
       "      <td>-0.526855</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1371077.0</td>\n",
       "      <td>1407828.0</td>\n",
       "      <td>sixpackistan</td>\n",
       "      <td>Term limits != one term.  It means, you can't ...</td>\n",
       "      <td>162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.004478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;=1 month</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50262.0</td>\n",
       "      <td>429393.0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.006943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>366334.0</td>\n",
       "      <td>379131.0</td>\n",
       "      <td>F_IT</td>\n",
       "      <td>Who voted the fuckers in? Thats right.... Amer...</td>\n",
       "      <td>92</td>\n",
       "      <td>0.027771</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2012</td>\n",
       "      <td>-0.636230</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8583.0</td>\n",
       "      <td>257214.00000000003</td>\n",
       "      <td>Goldenrule-er</td>\n",
       "      <td>Honestly though, forgive me for being so crass...</td>\n",
       "      <td>372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.691406</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>198059.0</td>\n",
       "      <td>248631.00000000003</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>It could also mean that you're a spoiled child.</td>\n",
       "      <td>47</td>\n",
       "      <td>0.006943</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>105719.0</td>\n",
       "      <td>154869.0</td>\n",
       "      <td>MdxBhmt</td>\n",
       "      <td>And it's also sad that some of those good peop...</td>\n",
       "      <td>155</td>\n",
       "      <td>0.131958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2cneq</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;=7 days</td>\n",
       "      <td>Comments</td>\n",
       "      <td>2012</td>\n",
       "      <td>-0.817383</td>\n",
       "      <td>3</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age_comment            age_post         author  \\\n",
       "0                 0.0                 0.0  TruthToPower1   \n",
       "1  29579.000000000004           1437407.0    wharpudding   \n",
       "2  29409.000000000004           1437237.0      [deleted]   \n",
       "3           1365677.0           1407916.0   sixpackistan   \n",
       "4           1371077.0           1407828.0   sixpackistan   \n",
       "5             50262.0            429393.0      [deleted]   \n",
       "6            366334.0            379131.0           F_IT   \n",
       "7              8583.0  257214.00000000003  Goldenrule-er   \n",
       "8            198059.0  248631.00000000003      [deleted]   \n",
       "9            105719.0            154869.0        MdxBhmt   \n",
       "\n",
       "                                                body  body_length  \\\n",
       "0  Congress is at 5% approval, and 86% say Congre...          101   \n",
       "1  Hey, you're right.  We could have gotten rid o...          132   \n",
       "2                                          [deleted]            9   \n",
       "3  There are no term limits and, no, they're not ...          269   \n",
       "4  Term limits != one term.  It means, you can't ...          162   \n",
       "5                                          [deleted]            9   \n",
       "6  Who voted the fuckers in? Thats right.... Amer...           92   \n",
       "7  Honestly though, forgive me for being so crass...          372   \n",
       "8    It could also mean that you're a spoiled child.           47   \n",
       "9  And it's also sad that some of those good peop...          155   \n",
       "\n",
       "   centrality_betweeness  centrality_closeness  centrality_degree  \\\n",
       "0               0.000000              0.319092           0.292480   \n",
       "1               0.000000              0.000000           0.001493   \n",
       "2               0.020828              0.000000           0.001493   \n",
       "3               0.000000              0.000000           0.001493   \n",
       "4               0.000000              0.002985           0.004478   \n",
       "5               0.006943              0.000000           0.001493   \n",
       "6               0.027771              0.001493           0.002985   \n",
       "7               0.000000              0.000000           0.001493   \n",
       "8               0.006943              0.001493           0.002985   \n",
       "9               0.131958              0.000000           0.001493   \n",
       "\n",
       "   centrality_eigenvector  centrality_page_rank  ...  quarter score  \\\n",
       "0                     1.0              0.551270  ...        1  2510   \n",
       "1                     0.0              0.000298  ...        1     0   \n",
       "2                     0.0              0.000298  ...        1     1   \n",
       "3                     0.0              0.000298  ...        1     1   \n",
       "4                     0.0              0.000775  ...        1     0   \n",
       "5                     0.0              0.000298  ...        1     1   \n",
       "6                     0.0              0.000537  ...        1     1   \n",
       "7                     0.0              0.000298  ...        1     1   \n",
       "8                     0.0              0.000537  ...        1     1   \n",
       "9                     0.0              0.000298  ...        1     2   \n",
       "\n",
       "  subreddit_id  time_group  time_label      type  year nlp_score nlp_group  \\\n",
       "0        2cneq           0   <=8 hours      Post  2012  0.726562         1   \n",
       "1        2cneq           4   <=1 month  Comments  2012  0.763184         1   \n",
       "2        2cneq           4   <=1 month  Comments  2012  0.000000         2   \n",
       "3        2cneq           4   <=1 month  Comments  2012 -0.526855         3   \n",
       "4        2cneq           4   <=1 month  Comments  2012  0.000000         2   \n",
       "5        2cneq           3    <=7 days  Comments  2012  0.000000         2   \n",
       "6        2cneq           3    <=7 days  Comments  2012 -0.636230         3   \n",
       "7        2cneq           3    <=7 days  Comments  2012  0.691406         1   \n",
       "8        2cneq           3    <=7 days  Comments  2012  0.000000         2   \n",
       "9        2cneq           3    <=7 days  Comments  2012 -0.817383         3   \n",
       "\n",
       "  nlp_label  \n",
       "0  Positive  \n",
       "1  Positive  \n",
       "2   Neutral  \n",
       "3  Negative  \n",
       "4   Neutral  \n",
       "5   Neutral  \n",
       "6  Negative  \n",
       "7  Positive  \n",
       "8   Neutral  \n",
       "9  Negative  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#conn = sqlite3.connect(\"test.db\")\n",
    "conn = sqlite3.connect(\"Data/obama_election.db\")\n",
    "df_test = pd.read_sql('''SELECT * FROM Reddit limit 10''', conn)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- posts: 2016 - 1318, 2012 - 1283 \n",
    "- observations: 2016 - 4496239, 2012 - 2044993 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
